{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlackJack\n",
    "\n",
    "File also uploaded at my [github - yashashwi-s](https://github.com/yashashwi-s/RLbook)\n",
    "\n",
    "## Introduction to Blackjack\n",
    "\n",
    "Blackjack, also known as 21, is a popular card game played in casinos. The objective of the game is to have a hand value that is closer to 21 than the dealer's hand without exceeding 21. \n",
    "\n",
    "### Basic Rules\n",
    "- **Card Values**: \n",
    "  - Number cards (2-10) are worth their face value.\n",
    "  - Face cards (King, Queen, Jack) are worth 10 points.\n",
    "  - Aces can be worth 1 or 11 points, whichever is more favorable for the player.\n",
    "\n",
    "### Gameplay\n",
    "1. **Initial Deal**: \n",
    "   - Both the player and the dealer are dealt two cards. \n",
    "   - The player’s cards are usually dealt face-up, while the dealer has one card face-up and one face-down.\n",
    "\n",
    "2. **Player's Turn**: \n",
    "   - The player has several actions they can take:\n",
    "     - **Stick**: End their turn and keep their current hand.\n",
    "     - **Hit**: Draw another card from the deck.\n",
    "     - **Double Down**: Double their bet, take exactly one more card, and then stick.\n",
    "     - **Surrender**: Forfeit the round and lose half of their bet (only allowed as the first action).\n",
    "\n",
    "3. **Dealer's Turn**: \n",
    "   - The dealer reveals their face-down card.\n",
    "   - The dealer must draw cards until their hand value is at least 17.\n",
    "\n",
    "4. **Winning Conditions**:\n",
    "   - The player wins if their hand is closer to 21 than the dealer's hand or if the dealer busts (exceeds 21).\n",
    "   - The player loses if they bust or if the dealer’s hand is closer to 21.\n",
    "   - If both the player and the dealer have the same hand value, it is a tie (push).\n",
    "\n",
    "### Our Implementation\n",
    "We will simulate this game in Python using OpenAI's Gym environment. The actions in our implementation are:\n",
    "- **Stick (0)**\n",
    "- **Hit (1)**\n",
    "- **Double Down (2)**\n",
    "- **Surrender (3)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Cards:\n",
    " Here we assume an infinite deck, so we can have one card any number of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawCard(np_random):\n",
    "    card = np_random.choice([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10])\n",
    "    return card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Hands:\n",
    "Getting the intial hand from randomised deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawHand(np_random):\n",
    "    return [drawCard(np_random), drawCard(np_random)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ace:\n",
    "This method check for presence of a usable ace, i.e., was their ace present in the initial withdrawal of hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ace(hand):\n",
    "    return 1 in hand and sum(hand) + 10 <= 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand Sum:\n",
    "This returns the the sum of all cards in the hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handSum(hand):\n",
    "    if Ace(hand):\n",
    "        return sum(hand) + 10\n",
    "    return sum(hand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bust:\n",
    "This function checks if the hand has gone bust, i.e., it  checks whether the sum of hand is more than 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bust(hand):\n",
    "    return sum(hand) > 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural:\n",
    "To check whether the initial hand has hit 21, which would result in a +1.5 reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural(hand):\n",
    "    return sorted(hand) == [1, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible:\n",
    "This checks whether we have the option to double down or surrender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible(hand, actions_taken):\n",
    "    return len(hand) == 2 and actions_taken == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Hands:\n",
    "To compare whose hand is better than the other person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareHands(a, b):\n",
    "    return float(a > b) - float(a < b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(hand):\n",
    "    return 0 if bust(hand) else handSum(hand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJackEnv(gym.Env):\n",
    "    def __init__(self, natural=False):\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(32), # Player's hand sum\n",
    "            spaces.Discrete(11), # Dealer's face-up card\n",
    "            spaces.Discrete(2),  # Usable ace\n",
    "            spaces.Discrete(2)   # Double down or surrender possibility\n",
    "        ))\n",
    "        self.seed()\n",
    "        self.natural = natural\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.actions_taken = 0\n",
    "        self.dealer_hand = drawHand(self.np_random)\n",
    "        self.player_hand = drawHand(self.np_random)\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return (handSum(self.player_hand), self.dealer_hand[0], \n",
    "                Ace(self.player_hand), possible(self.player_hand, self.actions_taken))\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        \n",
    "        if action == 0:  # Stick\n",
    "            done = True\n",
    "            while handSum(self.dealer_hand) < 17:\n",
    "                self.dealer_hand.append(drawCard(self.np_random))\n",
    "            reward = compareHands(score(self.player_hand), score(self.dealer_hand))\n",
    "            if natural(self.player_hand) and reward == 1:\n",
    "                reward = 1.5 if self.natural else 1\n",
    "        elif action == 1:  # Hit\n",
    "            self.player_hand.append(drawCard(self.np_random))\n",
    "            if bust(self.player_hand):\n",
    "                done = True\n",
    "                reward = -1\n",
    "            else:\n",
    "                done = False\n",
    "                reward = 0\n",
    "        elif action == 2:  # Double down\n",
    "            \n",
    "            self.player_hand.append(drawCard(self.np_random))\n",
    "            if bust(self.player_hand):\n",
    "                done = True\n",
    "                reward = -2\n",
    "            else:\n",
    "                while handSum(self.dealer_hand) < 17:\n",
    "                    self.dealer_hand.append(drawCard(self.np_random))\n",
    "                temp_reward = compareHands(score(self.player_hand), score(self.dealer_hand))\n",
    "                reward = 2 * temp_reward\n",
    "                done = True\n",
    "                \n",
    "        elif action == 3: # Surrender\n",
    "            assert len(self.player_hand) == 2\n",
    "            done = True\n",
    "            reward = -0.5\n",
    "\n",
    "        self.actions_taken += 1\n",
    "        return self._get_obs(), reward, done, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "\n",
    "The alorithm is taken from Sutton and Barto and is attached below.\n",
    "\n",
    "![Algorithm](./MCCES.png)\n",
    "\n",
    "## Monte Carlo Control with Exploring Starts\n",
    "\n",
    "Our model uses Monte Carlo methods for learning an optimal policy for playing Blackjack. Monte Carlo methods involve running many simulations of the game to gather data on the outcomes of different actions in different states. This data is then used to estimate the expected rewards for each action in each state.\n",
    "\n",
    "### Algorithm: Monte Carlo with Exploring Starts\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize the action-value function Q to arbitrary values.\n",
    "   - Initialize an empty list for returns for each state-action pair.\n",
    "\n",
    "2. **Policy**:\n",
    "   - Use an epsilon-greedy policy for action selection, which allows exploration of the state space by occasionally choosing random actions.\n",
    "\n",
    "3. **Generating Episodes**:\n",
    "   - Generate episodes (sequences of state-action-reward tuples) using the current policy.\n",
    "   - Ensure that all state-action pairs are explored by starting the episodes from random state-action pairs.\n",
    "\n",
    "4. **Updating Q-values**:\n",
    "   - For each episode, calculate the return (cumulative reward) for each state-action pair.\n",
    "   - Update the Q-values as the average of these returns.\n",
    "\n",
    "5. **Improving the Policy**:\n",
    "   - Use the updated Q-values to improve the policy, making it more likely to choose actions with higher expected rewards.\n",
    "\n",
    "### Algorithm Steps\n",
    "1. Initialize Q and returns.\n",
    "2. Repeat for a large number of episodes:\n",
    "   - Generate an episode using the current policy.\n",
    "   - For each state-action pair in the episode:\n",
    "     - Calculate the return.\n",
    "     - Update Q.\n",
    "     - Update the policy to be epsilon-greedy with respect to the new Q-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and Environment Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlackJackEnv()\n",
    "\n",
    "epsilon = 0.1\n",
    "gamma = 1.0\n",
    "num_episodes = 500000\n",
    "\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "Returns = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(policy):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        valid_actions = [0, 1, 2, 3] if possible(env.player_hand, env.actions_taken) else [0, 1]\n",
    "        action_probabilities = np.array([policy(state)[a] for a in valid_actions])\n",
    "        action_probabilities /= action_probabilities.sum()\n",
    "        action = np.random.choice(valid_actions, p=action_probabilities)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Initialization ($\\epsilon$-greedy):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy(Q, epsilon, num_actions):\n",
    "    def policy_fn(state):\n",
    "        valid_actions = [0, 1, 2, 3] if possible(env.player_hand, env.actions_taken) else [0, 1]\n",
    "        policy = np.zeros(num_actions)\n",
    "        for action in valid_actions:\n",
    "            policy[action] = epsilon / len(valid_actions)\n",
    "        best_action = valid_actions[np.argmax([Q[state][a] for a in valid_actions])]\n",
    "        policy[best_action] += (1.0 - epsilon)\n",
    "        return policy\n",
    "    return policy_fn\n",
    "\n",
    "policy = create_policy(Q, epsilon, env.action_space.n)\n",
    "average_returns = []\n",
    "average_returns_last_10000 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Q-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000/500000: Average Return = -0.219, Last 10000 Average = -0.219\n",
      "Episode 20000/500000: Average Return = -0.1885, Last 10000 Average = -0.20375\n",
      "Episode 30000/500000: Average Return = -0.139, Last 10000 Average = -0.18216666666666667\n",
      "Episode 40000/500000: Average Return = -0.164, Last 10000 Average = -0.177625\n",
      "Episode 50000/500000: Average Return = -0.1415, Last 10000 Average = -0.1704\n",
      "Episode 60000/500000: Average Return = -0.118, Last 10000 Average = -0.16166666666666665\n",
      "Episode 70000/500000: Average Return = -0.1055, Last 10000 Average = -0.15364285714285714\n",
      "Episode 80000/500000: Average Return = -0.1225, Last 10000 Average = -0.14975\n",
      "Episode 90000/500000: Average Return = -0.131, Last 10000 Average = -0.14766666666666667\n",
      "Episode 100000/500000: Average Return = -0.152, Last 10000 Average = -0.14809999999999998\n",
      "Episode 110000/500000: Average Return = -0.117, Last 10000 Average = -0.13789999999999997\n",
      "Episode 120000/500000: Average Return = -0.0835, Last 10000 Average = -0.1274\n",
      "Episode 130000/500000: Average Return = -0.109, Last 10000 Average = -0.12439999999999998\n",
      "Episode 140000/500000: Average Return = -0.086, Last 10000 Average = -0.11660000000000001\n",
      "Episode 150000/500000: Average Return = -0.141, Last 10000 Average = -0.11655\n",
      "Episode 160000/500000: Average Return = -0.09, Last 10000 Average = -0.11375000000000002\n",
      "Episode 170000/500000: Average Return = -0.0935, Last 10000 Average = -0.11255\n",
      "Episode 180000/500000: Average Return = -0.145, Last 10000 Average = -0.11479999999999999\n",
      "Episode 190000/500000: Average Return = -0.1305, Last 10000 Average = -0.11474999999999999\n",
      "Episode 200000/500000: Average Return = -0.1355, Last 10000 Average = -0.1131\n",
      "Episode 210000/500000: Average Return = -0.1465, Last 10000 Average = -0.11605000000000001\n",
      "Episode 220000/500000: Average Return = -0.086, Last 10000 Average = -0.1163\n",
      "Episode 230000/500000: Average Return = -0.144, Last 10000 Average = -0.11979999999999999\n",
      "Episode 240000/500000: Average Return = -0.126, Last 10000 Average = -0.1238\n",
      "Episode 250000/500000: Average Return = -0.1305, Last 10000 Average = -0.12275\n",
      "Episode 260000/500000: Average Return = -0.0445, Last 10000 Average = -0.1182\n",
      "Episode 270000/500000: Average Return = -0.123, Last 10000 Average = -0.12115000000000001\n",
      "Episode 280000/500000: Average Return = -0.1265, Last 10000 Average = -0.1193\n",
      "Episode 290000/500000: Average Return = -0.09, Last 10000 Average = -0.11525\n",
      "Episode 300000/500000: Average Return = -0.123, Last 10000 Average = -0.11399999999999999\n",
      "Episode 310000/500000: Average Return = -0.1115, Last 10000 Average = -0.1105\n",
      "Episode 320000/500000: Average Return = -0.149, Last 10000 Average = -0.11679999999999999\n",
      "Episode 330000/500000: Average Return = -0.061, Last 10000 Average = -0.1085\n",
      "Episode 340000/500000: Average Return = -0.097, Last 10000 Average = -0.1056\n",
      "Episode 350000/500000: Average Return = -0.1015, Last 10000 Average = -0.10269999999999999\n",
      "Episode 360000/500000: Average Return = -0.1405, Last 10000 Average = -0.1123\n",
      "Episode 370000/500000: Average Return = -0.033, Last 10000 Average = -0.10329999999999999\n",
      "Episode 380000/500000: Average Return = -0.0265, Last 10000 Average = -0.09330000000000001\n",
      "Episode 390000/500000: Average Return = -0.1095, Last 10000 Average = -0.09525\n",
      "Episode 400000/500000: Average Return = -0.1665, Last 10000 Average = -0.0996\n",
      "Episode 410000/500000: Average Return = -0.134, Last 10000 Average = -0.10185\n",
      "Episode 420000/500000: Average Return = -0.1075, Last 10000 Average = -0.09770000000000001\n",
      "Episode 430000/500000: Average Return = -0.0695, Last 10000 Average = -0.09855\n",
      "Episode 440000/500000: Average Return = -0.1345, Last 10000 Average = -0.10230000000000002\n",
      "Episode 450000/500000: Average Return = -0.113, Last 10000 Average = -0.10345\n",
      "Episode 460000/500000: Average Return = -0.0405, Last 10000 Average = -0.09345\n",
      "Episode 470000/500000: Average Return = -0.085, Last 10000 Average = -0.09864999999999999\n",
      "Episode 480000/500000: Average Return = -0.0785, Last 10000 Average = -0.10385\n",
      "Episode 490000/500000: Average Return = -0.1535, Last 10000 Average = -0.10825\n",
      "Episode 500000/500000: Average Return = -0.074, Last 10000 Average = -0.09899999999999999\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_episodes):\n",
    "    episode = generate_episode(policy)\n",
    "    G = 0\n",
    "    visited_state_action_pairs = set()\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = gamma * G + reward\n",
    "        if (state, action) not in visited_state_action_pairs:\n",
    "            Returns[(state, action)].append(G)\n",
    "            Q[state][action] = np.mean(Returns[(state, action)])\n",
    "            visited_state_action_pairs.add((state, action))\n",
    "    policy = create_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    if (i + 1) % 10000 == 0:\n",
    "        average_return = np.mean([sum([reward for (_, _, reward) in generate_episode(policy)]) for _ in range(1000)])\n",
    "        average_returns.append(average_return)\n",
    "        if len(average_returns) > 1:\n",
    "            avg_last_10000 = np.mean(average_returns[-10:])\n",
    "        else:\n",
    "            avg_last_10000 = average_return\n",
    "        average_returns_last_10000.append(avg_last_10000)\n",
    "        print(f'Episode {i+1}/{num_episodes}: Average Return = {average_return}, Last 10000 Average = {avg_last_10000}')\n",
    "        \n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = [(sum_hand, dealer_card, usable_ace, double_down) for sum_hand in range(4, 22)\n",
    "               for dealer_card in range(1, 11) for usable_ace in [False, True] for double_down in [False, True]]\n",
    "\n",
    "policy_matrix = np.zeros((32, 11, 2, 2), dtype=int)\n",
    "for (player_sum, dealer_card, usable_ace, double_down) in state_space:\n",
    "    state = (player_sum, dealer_card, usable_ace, double_down)\n",
    "    if state in Q:\n",
    "        best_action = np.argmax(Q[state])\n",
    "        policy_matrix[player_sum, dealer_card, usable_ace, double_down] = best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating final Policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final average return over 100000 games: -0.046725\n",
      "Number of hits: 31286.0\n",
      "Number of stands: 64845.0\n",
      "Number of double downs: 10826.0\n",
      "Number of surrenders: 17993.0\n"
     ]
    }
   ],
   "source": [
    "num_test_episodes = 100000\n",
    "total_rewards = 0\n",
    "action_counts = np.zeros(4)\n",
    "\n",
    "for _ in range(num_test_episodes):\n",
    "    state = env.reset()\n",
    "    episode_rewards = 0\n",
    "    while True:\n",
    "        valid_actions = [0, 1, 2, 3] if possible(env.player_hand, env.actions_taken) else [0, 1]\n",
    "        action = np.argmax([Q[state][a] for a in valid_actions])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        action_counts[action] += 1\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards += episode_rewards\n",
    "\n",
    "average_return = total_rewards / num_test_episodes\n",
    "print(f\"Final average return over {num_test_episodes} games: {average_return}\")\n",
    "print(f\"Number of hits: {action_counts[1]}\")\n",
    "print(f\"Number of stands: {action_counts[0]}\")\n",
    "print(f\"Number of double downs: {action_counts[2]}\")\n",
    "print(f\"Number of surrenders: {action_counts[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "Even after such training and palying ```1,00,000``` games, our policy could not return a positive value which probably tells us to stay away from gambling :)\n",
    "\n",
    "A personal note: I tried a lot of different stuff to add another action ```split``` but that simply wouldn't work and the code was filled with bugs so at the end I finally decided to remove it to save some troubles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
