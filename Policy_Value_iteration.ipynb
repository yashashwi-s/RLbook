{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEQJwsfoe1bGykoOMZQbho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashashwi-s/RLbook/blob/main/Policy_Value_iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import io\n",
        "import sys\n",
        "from gym import Env, spaces"
      ],
      "metadata": {
        "id": "6a0Kn-bhjCLH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions\n",
        "\n",
        "UP = 0\n",
        "RIGHT = 1\n",
        "DOWN = 2\n",
        "LEFT = 3"
      ],
      "metadata": {
        "id": "6swHxuR1jRB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GridworldEnv(Env):\n",
        "  metadata = {'render.modes': ['human', 'ansi']}\n",
        "\n",
        "  def __init__(self, shape=(4,4)):\n",
        "    self.shape = shape\n",
        "    self.nS = np.prod(shape)\n",
        "    self.nA = 4\n",
        "\n",
        "    self.max_y = shape[0]\n",
        "    self.max_x = shape[1]\n",
        "\n",
        "    self.action_space = spaces.Discrete(self.nA)\n",
        "    self.observation_space = spaces.Discrete(self.nS)\n",
        "\n",
        "    self.state = None\n",
        "    self.start_state = self.nS // 2\n",
        "\n",
        "    self.P = self._create_transition_probabilities()\n",
        "\n",
        "    self.isd = np.ones(self.nS) / self.nS\n",
        "\n",
        "  def _create_transition_probabilities(self):\n",
        "    P = {}\n",
        "    grid = np.arange(self.nS).reshape(self.shape)\n",
        "    it = np.nditer(grid, flags=['multi_index'])\n",
        "\n",
        "    while not it.finished:\n",
        "      s = it.iterindex\n",
        "      y, x = it.multi_index\n",
        "\n",
        "      P[s] = {a: [] for a in range(self.nA)}\n",
        "      is_done = lambda s: s == 0 or s == (self.nS - 1)\n",
        "      reward = 0.0 if is_done(s) else -1.0\n",
        "\n",
        "      if is_done(s):\n",
        "        for a in range(self.nA):\n",
        "            P[s][a] = [(1.0, s, reward, True)]\n",
        "      else:\n",
        "        ns_up = s if y == 0 else s - self.max_x\n",
        "        ns_right = s if x == (self.max_x - 1) else s + 1\n",
        "        ns_down = s if y == (self.max_y - 1) else s + self.max_x\n",
        "        ns_left = s if x == 0 else s - 1\n",
        "\n",
        "        P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n",
        "        P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n",
        "        P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n",
        "        P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n",
        "\n",
        "      it.iternext()\n",
        "\n",
        "    return P\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = self.start_state\n",
        "    return self.state\n",
        "\n",
        "  def step(self, action):\n",
        "    transitions = self.P[self.state][action]\n",
        "    prob, next_state, reward, done = transitions[0]\n",
        "    self.state = next_state\n",
        "    return next_state, reward, done, {}\n",
        "\n",
        "  def render(self, mode='human'):\n",
        "    if mode not in self.metadata['render.modes']:\n",
        "      raise ValueError(\"Invalid render mode: {}\".format(mode))\n",
        "\n",
        "    outfile = io.StringIO() if mode == 'ansi' else sys.stdout\n",
        "\n",
        "    grid = np.arange(self.nS).reshape(self.shape)\n",
        "    it = np.nditer(grid, flags=['multi_index'])\n",
        "    while not it.finished:\n",
        "      s = it.iterindex\n",
        "      y, x = it.multi_index\n",
        "\n",
        "      if self.state == s:\n",
        "          output = \" x \"\n",
        "      elif s == 0 or s == self.nS - 1:\n",
        "          output = \" T \"\n",
        "      else:\n",
        "          output = \" o \"\n",
        "\n",
        "      if x == 0:\n",
        "          output = output.lstrip()\n",
        "      if x == self.shape[1] - 1:\n",
        "          output = output.rstrip()\n",
        "\n",
        "      outfile.write(output)\n",
        "\n",
        "      if x == self.shape[1] - 1:\n",
        "          outfile.write(\"\\n\")\n",
        "\n",
        "      it.iternext()\n",
        "\n",
        "    if mode == 'ansi':\n",
        "      return outfile.getvalue()"
      ],
      "metadata": {
        "id": "uZC53CNXjaeC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridworldEnv()"
      ],
      "metadata": {
        "id": "QtmJjOGelvNK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#action estimation\n",
        "\n",
        "def action_estimation(env, state, V, gamma):\n",
        "  A = np.zeros(env.nA)\n",
        "  for a in range (env.nA):\n",
        "    for prob, next_state, reward, done in env.P[state][a]:\n",
        "      A[a] += prob*(reward + gamma*V[next_state])\n",
        "\n",
        "  return A"
      ],
      "metadata": {
        "id": "9ZvRROlJnODb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_action(A):\n",
        "  best_Action = np.argmax(A)\n",
        "  return best_Action, A[best_Action]"
      ],
      "metadata": {
        "id": "jx2ZIPTJodL7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_estimation(env, V, gamma):\n",
        "  policy = np.zeros([env.nS, env.nA])\n",
        "\n",
        "  while True:\n",
        "    policy_stable = True\n",
        "    for s in range(env.nS):\n",
        "      A = action_estimation(env, s, V, gamma)\n",
        "      chosen_a = np.argmax(policy[s])\n",
        "      best_action, _ = get_best_action(A)\n",
        "\n",
        "      if best_action != chosen_a:\n",
        "        policy_stable = False\n",
        "      policy[s] = np.eye(env.nA)[best_action]\n",
        "\n",
        "    if policy_stable:\n",
        "      return policy\n"
      ],
      "metadata": {
        "id": "o5fMk71TowUb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration (env, policy_estimator= policy_estimation, theta = 0.0001, gamma = 1, max_iterations = 1000):\n",
        "  V = np.zeros(env.nS)\n",
        "  p = policy_estimator(env, V, gamma)\n",
        "  found_optimal = False\n",
        "\n",
        "  for iteration in range(max_iterations):\n",
        "    delta = 0\n",
        "    for s in range(env.nS):\n",
        "      best_Action_Value = 0\n",
        "      for a, action_prob in enumerate(p[s]):\n",
        "        for prob, next_state, reward, done in env.P[s][a]:\n",
        "          best_Action_Value += action_prob * prob * (reward + gamma*V[next_state])\n",
        "\n",
        "      delta = max(delta ,np.abs(best_Action_Value - V[s]))\n",
        "      V[s] = best_Action_Value\n",
        "\n",
        "    if delta < theta:\n",
        "      found_optimal = True\n",
        "      break\n",
        "\n",
        "    p = policy_estimator(env, V, gamma)\n",
        "    print(p)\n",
        "\n",
        "  return p,V"
      ],
      "metadata": {
        "id": "PkM0cJg2pppb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, v = policy_iteration(env)\n",
        "print(policy)\n",
        "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
        "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
        "print(\"\")\n",
        "\n",
        "print(\"Value Function:\")\n",
        "print(v)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Reshaped Grid Value Function:\")\n",
        "print(v.reshape(env.shape))\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HHYMe70drCxM",
        "outputId": "a6307e38-7aa0-426c-982d-76acf8589bce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
            "[[0 3 3 2]\n",
            " [0 0 0 2]\n",
            " [0 0 1 2]\n",
            " [0 1 1 0]]\n",
            "\n",
            "Value Function:\n",
            "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
            "\n",
            "Reshaped Grid Value Function:\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, theta=0.0001, discount_factor=1.0, max_iterations=1000):\n",
        "  V = np.zeros(env.nS)\n",
        "\n",
        "  for iteration in range(max_iterations):\n",
        "      # Stopping condition\n",
        "      delta = 0\n",
        "\n",
        "      # Update each state...\n",
        "      for s in range(env.nS):\n",
        "          # Do a one-step lookahead to find the best action\n",
        "          A = action_estimation(env, s, V, discount_factor)\n",
        "\n",
        "          best_action, best_action_value = get_best_action(A)\n",
        "\n",
        "          # Calculate delta across all states seen so far\n",
        "          delta = max(delta, np.abs(best_action_value - V[s]))\n",
        "\n",
        "          # Update the value function. Ref: Sutton book eq. 4.10.\n",
        "          V[s] = best_action_value\n",
        "\n",
        "        # Check if we can stop\n",
        "      if delta < theta:\n",
        "        break\n",
        "\n",
        "  policy = policy_estimation(env, V, discount_factor)\n",
        "\n",
        "  return policy, V"
      ],
      "metadata": {
        "id": "gfZhKt2mtxzT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, v = value_iteration(env)\n",
        "\n",
        "print(\"Policy Probability Distribution:\")\n",
        "print(policy)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
        "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
        "print(\"\")\n",
        "\n",
        "print(\"Value Function:\")\n",
        "print(v)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Reshaped Grid Value Function:\")\n",
        "print(v.reshape(env.shape))\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEDFkb3tt_-c",
        "outputId": "3a01840f-b8ec-4a00-9a68-55e4107c7d4b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Probability Distribution:\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "\n",
            "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
            "[[0 3 3 2]\n",
            " [0 0 0 2]\n",
            " [0 0 1 2]\n",
            " [0 1 1 0]]\n",
            "\n",
            "Value Function:\n",
            "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
            "\n",
            "Reshaped Grid Value Function:\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}